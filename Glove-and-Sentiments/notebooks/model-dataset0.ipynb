{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fea4a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:35:20.789646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 22:35:21.280877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-17 22:35:21.747865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 22:35:21.748121: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "from torchtext.vocab import GloVe, FastText\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7142ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     2588\n",
      "positive    1287\n",
      "negative     125\n",
      "Name: Label, dtype: int64\n",
      "--------------------------------------------------------\n",
      "Number of reviews here in dataset are : 4000\n"
     ]
    }
   ],
   "source": [
    "# Different Classes and their counts\n",
    "data = pd.read_excel('/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/Datasets/ClassificationDataset0.xlsx')\n",
    "print(data[\"Label\"].value_counts())\n",
    "print(\"--------------------------------------------------------\")\n",
    "size = len(data[\"Review\"])\n",
    "print(f\"Number of reviews here in dataset are : {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f15219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_path = '/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/data/Dataset0.csv'\n",
    "\n",
    "# def making_new_dataset(data):\n",
    "#     data.to_csv(pos_path, index = False, header= False,\n",
    "#           encoding = \"latin-1\", columns = ['Positive Review'])\n",
    "    \n",
    "#     positive_set = open(pos_path, \"r\", encoding=\"latin-1\").read()\n",
    "#     negative_set = open(neg_path, \"r\", encoding=\"latin-1\").read()\n",
    "    \n",
    "#     pos_set = positive_set.split(\"\\n\")[:-1]\n",
    "#     neg_set = negative_set.split(\"\\n\")[:-1]\n",
    "    \n",
    "#     #print(len(positive_data), len(negative_data))\n",
    "    \n",
    "#     return pos_set, neg_set\n",
    "\n",
    "# positive_data, negative_data = making_new_dataset(data) \n",
    "csv = data.to_csv(csv_data_path, index = False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a928e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library that contains punctuation\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "#defining the function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe18fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "pos_set = []\n",
    "neg_set = []\n",
    "neutral_set = []\n",
    "with open(\"/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/data/Dataset0.csv\", encoding='utf-8') as csvf:\n",
    "    data = csv.DictReader(csvf)\n",
    "    #data['Review'] = data['Review'].apply(lambda x:remove_punctuation(x))\n",
    "    for rows in data:\n",
    "        # Removing punctuations\n",
    "        chars_to_remove = [ '+', '#', '¡', '§', '…','‘', '’', '¿', '«', '»', '¨', '%', '-', '“', '”', '--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '/']\n",
    "        \n",
    "        review = rows['Review'].replace('<br />', \" \", -1)\n",
    "        review = review.replace('´', \"'\", -1)\n",
    "        for char in chars_to_remove:\n",
    "            review = review.replace(char, \" \", -1)\n",
    "        \n",
    "        \n",
    "        if rows['Label'] == 'positive':\n",
    "            pos_set.append(rows['Review'])\n",
    "        elif rows['Label'] == 'negative':\n",
    "            neg_set.append(rows['Review'])\n",
    "        else:\n",
    "            neutral_set.append(rows['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662ff037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "\n",
    "def preprocess_tokenized_reviewText_dataset0(pos, neg, neutral):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    data_set = []\n",
    "    vocab = []\n",
    "    chars_to_remove = ['--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '.', ',']\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    \n",
    "    for line in pos:\n",
    "        # Tokenizes the input text into words\n",
    "        tokens = tokenizer(line)\n",
    "\n",
    "        data_set.append((tokens, 1))\n",
    "        # Adds the extracted words to a list\n",
    "        vocab.extend(tokens)\n",
    "    print(f\"--- Positive with label 1 ---- Finished ---\")\n",
    "    \n",
    "    for line in neg:\n",
    "        # Tokenizes the input text into words\n",
    "        tokens = tokenizer(line)\n",
    "\n",
    "        data_set.append((tokens, 0))\n",
    "        # Adds the extracted words to a list\n",
    "        vocab.extend(tokens)\n",
    "    print(f\"--- Negative with label 0 ----  Finished ---\")\n",
    "    \n",
    "    for line in neutral:\n",
    "        # Tokenizes the input text into words\n",
    "        tokens = tokenizer(line)\n",
    "\n",
    "        data_set.append((tokens, 2))\n",
    "        # Adds the extracted words to a list\n",
    "        vocab.extend(tokens)\n",
    "    print(f\"--- Neutral with label 2 ---- Finished ---\")\n",
    "    \n",
    "    # Stores all the unique words in the dataset and their frequencies\n",
    "    vocabulary = {}\n",
    "\n",
    "    # Calculates the frequency of each unique word in the vocabulary\n",
    "    for word in vocab:\n",
    "        if word in vocabulary:\n",
    "            vocabulary[word] += 1\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "\n",
    "    print(\"Number of unique words in the vocabulary: \", len(vocabulary))\n",
    "    return data_set, vocabulary\n",
    "\n",
    "def sort_key(s):\n",
    "    return len(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81010d0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Positive with label 1 ---- Finished ---\n",
      "--- Negative with label 0 ----  Finished ---\n",
      "--- Neutral with label 2 ---- Finished ---\n",
      "Number of unique words in the vocabulary:  9542\n",
      "Length of Complete concatenated Dataset of reviews : 4000\n"
     ]
    }
   ],
   "source": [
    "data, vocabulary = preprocess_tokenized_reviewText_dataset0(pos_set, neg_set, neutral_set)\n",
    "print(f\"Length of Complete concatenated Dataset of reviews : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e02c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('temp.txt', 'w') as fp:\n",
    "#     fp.write('\\n'.join('%s %s' % x for x in data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da211fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9542\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize(corpus, ids_vocab):\n",
    "    \"\"\"\n",
    "        Converts words in the dataset to integer tokens\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    for line, sentiment in corpus:\n",
    "        new_line = []\n",
    "        for i, word in enumerate(line):\n",
    "            if word in ids_vocab and (i == 0 or word != line[i-1]):\n",
    "                new_line.append(ids_vocab[word])\n",
    "\n",
    "        new_line = torch.Tensor(new_line).long()\n",
    "        tokenized_corpus.append((new_line, sentiment))\n",
    "\n",
    "    return tokenized_corpus\n",
    "\n",
    "def assign_ids_to_words(vocab):\n",
    "    # Stores the integer token for each unique word in the vocabulary\n",
    "    ids_vocab = {}\n",
    "\n",
    "    id = 0\n",
    "    # Assigns words in the vocabulary to integer tokens\n",
    "    for word, v in vocabulary.items():\n",
    "        ids_vocab[word] = id\n",
    "        id += 1\n",
    "    \n",
    "    return ids_vocab\n",
    "\n",
    "ids_vocab = assign_ids_to_words(vocabulary)\n",
    "print(len(ids_vocab))\n",
    "#print(ids_vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19d5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_corpus = tokenize(data, ids_vocab)\n",
    "#token_corpus_test = tokenize(test_set, ids_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ca120bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1739\n"
     ]
    }
   ],
   "source": [
    "# Loading the embedding matrix\n",
    "emb_dim = 300\n",
    "\n",
    "embeds = torch.zeros(len(ids_vocab) + 1, emb_dim)\n",
    "\n",
    "n = 0\n",
    "for token, idx in ids_vocab.items():\n",
    "    embeds[idx] = global_vectors[token]\n",
    "\n",
    "    if sum(embeds[idx]) == 0:\n",
    "        embeds[idx] = torch.rand(300)\n",
    "        n+=1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e6dc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9543, 300])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf6140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''\n",
    "        For Reproducibility: Sets the seed of the entire notebook.\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Sets a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "def split_indices_tokenized(num_values, percentage):\n",
    "\n",
    "    # Determine size of Validation set\n",
    "    val_size = int(percentage * num_values)\n",
    "\n",
    "    # Create random permutation of 0 to num_values-1\n",
    "    idxs = np.random.permutation(num_values)\n",
    "    return np.sort(idxs[val_size:]), np.sort(idxs[:val_size])\n",
    "\n",
    "set_seed(1)\n",
    "train_pos_indices, val_pos_indices = split_indices_tokenized(len(pos_set), 0.1)\n",
    "train_neg_indices, val_neg_indices = split_indices_tokenized(len(neg_set), 0.1)\n",
    "train_neutral_indices, val_neutral_indices = split_indices_tokenized(len(neutral_set), 0.1)\n",
    "#+len(pos_set)-1\n",
    "train_indices = np.concatenate((train_pos_indices, train_neg_indices, train_neutral_indices))\n",
    "val_indices = np.concatenate((val_pos_indices, val_neg_indices, train_neutral_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b999c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3602, 2470)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices), len(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da2fae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ----------- Batching the data -----------\n",
    "def collate_fn(instn):\n",
    "\n",
    "    sentence = [x[0] for x in instn]\n",
    "\n",
    "    # Pre padding\n",
    "    sen_len = [len(x[0]) for x in instn]\n",
    "    max_len = max(sen_len)\n",
    "\n",
    "    padded_sent = torch.zeros(1, max_len)\n",
    "    sentence_pad = [torch.cat((torch.zeros(max_len-len(x[0])), x[0]), dim=0) for x in instn]\n",
    "    \n",
    "    for i in sentence_pad:\n",
    "        padded_sent = torch.cat((padded_sent, i.unsqueeze(dim=0)), dim=0)\n",
    "    padded_sent = padded_sent[1:].long()\n",
    "\n",
    "    # Post padding\n",
    "    #padded_sent = pad_sequence(sentence, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = torch.Tensor([x[1] for x in instn])\n",
    "\n",
    "    return (padded_sent, labels)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_sampler   = SubsetRandomSampler(train_indices)\n",
    "train_loader    = DataLoader(token_corpus, batch_size, sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "val_sampler     = SubsetRandomSampler(val_indices)\n",
    "val_loader      = DataLoader(token_corpus, batch_size, sampler=val_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ba3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923069e",
   "metadata": {},
   "source": [
    "## DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fadaf5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(600, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 2048)\n",
    "        self.linear3 = nn.Linear(2048, 512)\n",
    "        self.linear4 = nn.Linear(512, 64)\n",
    "        self.linear5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, Xb, tsne = False):\n",
    "        x = self.linear1(Xb)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.linear3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear4(x)\n",
    "        \n",
    "        if tsne == True:\n",
    "            return x\n",
    "            \n",
    "        x = F.relu(x)\n",
    "        x = self.linear5(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# ----------- Model -----------\n",
    "class BILSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeds):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeds, padding_idx=0)\n",
    "\n",
    "        self.gru = nn.GRU(input_size = 300, hidden_size = 128, num_layers = 2, batch_first = True, bidirectional = True, dropout=0.5)\n",
    "\n",
    "        self.lin1 = nn.Linear(256, 64)\n",
    "        self.lin2 = nn.Linear(64, 3)\n",
    "\n",
    "        self.lin3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        xe = self.embeddings(xb)\n",
    "        out, y = self.gru(xe)\n",
    "        \n",
    "        x = self.lin3(out).squeeze(dim=-1)\n",
    "        x = torch.softmax(x, dim=-1).unsqueeze(dim=1)\n",
    "        x = torch.bmm(x, out).squeeze(dim=1)              # Weighted average\n",
    "\n",
    "#         #x = torch.cat((x, y[2][ :, :], y[3][ :, :]), dim = 1) # Tried concatenating the representation with hidden units - got similar results\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9005e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb, yb in train_loader:\n",
    "    eg = xb\n",
    "    eg_y = yb\n",
    "    break\n",
    "eg_y = eg_y.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c899b032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3470, 0.3366, 0.3164],\n",
       "         [0.3459, 0.3353, 0.3187],\n",
       "         [0.3423, 0.3387, 0.3190],\n",
       "         [0.3398, 0.3399, 0.3202],\n",
       "         [0.3375, 0.3388, 0.3237],\n",
       "         [0.3448, 0.3351, 0.3201],\n",
       "         [0.3406, 0.3398, 0.3196],\n",
       "         [0.3422, 0.3403, 0.3175],\n",
       "         [0.3520, 0.3358, 0.3122],\n",
       "         [0.3469, 0.3342, 0.3189],\n",
       "         [0.3389, 0.3385, 0.3225],\n",
       "         [0.3399, 0.3412, 0.3189],\n",
       "         [0.3439, 0.3356, 0.3206],\n",
       "         [0.3404, 0.3366, 0.3230],\n",
       "         [0.3437, 0.3316, 0.3248],\n",
       "         [0.3441, 0.3361, 0.3198],\n",
       "         [0.3449, 0.3369, 0.3182],\n",
       "         [0.3403, 0.3412, 0.3184],\n",
       "         [0.3428, 0.3372, 0.3201],\n",
       "         [0.3413, 0.3353, 0.3234],\n",
       "         [0.3423, 0.3374, 0.3203],\n",
       "         [0.3456, 0.3368, 0.3176],\n",
       "         [0.3396, 0.3374, 0.3230],\n",
       "         [0.3411, 0.3437, 0.3152],\n",
       "         [0.3420, 0.3369, 0.3212],\n",
       "         [0.3444, 0.3349, 0.3207],\n",
       "         [0.3413, 0.3370, 0.3217],\n",
       "         [0.3435, 0.3346, 0.3220],\n",
       "         [0.3429, 0.3346, 0.3225],\n",
       "         [0.3422, 0.3413, 0.3165],\n",
       "         [0.3445, 0.3377, 0.3178],\n",
       "         [0.3437, 0.3381, 0.3182],\n",
       "         [0.3435, 0.3356, 0.3210],\n",
       "         [0.3438, 0.3359, 0.3203],\n",
       "         [0.3427, 0.3416, 0.3157],\n",
       "         [0.3434, 0.3383, 0.3183],\n",
       "         [0.3482, 0.3325, 0.3193],\n",
       "         [0.3416, 0.3380, 0.3204],\n",
       "         [0.3459, 0.3345, 0.3197],\n",
       "         [0.3430, 0.3350, 0.3219],\n",
       "         [0.3443, 0.3407, 0.3151],\n",
       "         [0.3409, 0.3390, 0.3201],\n",
       "         [0.3442, 0.3369, 0.3188],\n",
       "         [0.3406, 0.3390, 0.3204],\n",
       "         [0.3436, 0.3354, 0.3210],\n",
       "         [0.3506, 0.3337, 0.3157],\n",
       "         [0.3437, 0.3399, 0.3164],\n",
       "         [0.3396, 0.3369, 0.3236],\n",
       "         [0.3441, 0.3367, 0.3192],\n",
       "         [0.3430, 0.3399, 0.3170],\n",
       "         [0.3432, 0.3412, 0.3156],\n",
       "         [0.3411, 0.3424, 0.3165],\n",
       "         [0.3443, 0.3400, 0.3157],\n",
       "         [0.3436, 0.3351, 0.3213],\n",
       "         [0.3446, 0.3362, 0.3193],\n",
       "         [0.3466, 0.3385, 0.3149],\n",
       "         [0.3410, 0.3413, 0.3177],\n",
       "         [0.3411, 0.3393, 0.3196],\n",
       "         [0.3446, 0.3356, 0.3197],\n",
       "         [0.3401, 0.3368, 0.3231],\n",
       "         [0.3463, 0.3334, 0.3203],\n",
       "         [0.3407, 0.3363, 0.3230],\n",
       "         [0.3399, 0.3393, 0.3208],\n",
       "         [0.3436, 0.3376, 0.3188]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "         1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = BILSTM(embeds)\n",
    "yi = model1(eg.type(torch.LongTensor))\n",
    "print(yi.shape)\n",
    "yi, eg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e5a2976d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1001, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn_c(yi, eg_y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b2408d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BILSTM(embeds)\n",
    "model.to(device)\n",
    "opt_c = torch.optim.Adam(model.parameters(), lr = 0.001) # Same as Adam with weight decay = 0.001\n",
    "loss_fn_c = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e5af419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BILSTM(embeds)\n",
    "yi = model1(eg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d56c5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "21f89c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [00:00<00:00, 347.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Training Loss:  0.871560581943445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:00<00:00, 675.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  1.0177391370137532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m         val_pred\u001b[38;5;241m.\u001b[39mextend(y_hat\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, val_epoch_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader))\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(val_labels, val_pred)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ep \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m prev_val_loss \u001b[38;5;241m-\u001b[39m val_epoch_loss \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.015\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/metrics/_classification.py:95\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     92\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     97\u001b[0m             type_true, type_pred\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    102\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "# ----------- Main Training Loop -----------\n",
    "max_epoch = 10\n",
    "\n",
    "best_test_acc = 0\n",
    "for ep in range(max_epoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for xb, yb in tqdm(train_loader):\n",
    "        yb = yb.type(torch.LongTensor)\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        y_hat = model(xb)\n",
    "        loss = loss_fn_c(y_hat.squeeze(), yb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt_c.step()\n",
    "\n",
    "        opt_c.zero_grad()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "        epoch_loss += float(loss)\n",
    "\n",
    "    print(\"Epoch: \", ep+1, \" Training Loss: \", epoch_loss/len(train_loader))\n",
    "\n",
    "\n",
    "    #----------- Validation -----------\n",
    "\n",
    "    val_labels = []\n",
    "    val_pred = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader):\n",
    "            yb = yb.type(torch.LongTensor)\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_hat = model(xb)\n",
    "            loss = loss_fn_c(y_hat.squeeze(), yb)\n",
    "\n",
    "            val_epoch_loss += float(loss)\n",
    "\n",
    "            val_labels.extend(torch.round(yb).cpu().detach().numpy())\n",
    "            val_pred.extend(y_hat.round().cpu().detach().numpy())\n",
    "\n",
    "    print(\"Validation loss: \", val_epoch_loss/len(val_loader))\n",
    "    print(\"Validation accuracy: \", accuracy_score(val_labels, val_pred)*100)\n",
    "\n",
    "    if ep > 5 and prev_val_loss - val_epoch_loss > 0.015:\n",
    "        print(\"Saving Model\")\n",
    "        torch.save(model.state_dict(), \"GRU_Model_Dataset0.pt\")\n",
    "    \n",
    "    prev_val_loss = val_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3115891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcceb40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
