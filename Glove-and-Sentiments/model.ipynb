{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259efd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-17 07:36:07.340117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 07:36:07.800011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-17 07:36:08.314172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 07:36:08.314431: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from en-core-web-md==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data/home/ayyoobmohd/miniconda3/envs/DL/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchtext\n",
    "#!pip install openpyxl\n",
    "#!pip install nltk\n",
    "#!pip install spacy\n",
    "!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a62d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/home/ayyoobmohd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /data/home/ayyoobmohd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82589959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 07:36:18.870431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 07:36:19.408709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-17 07:36:19.867679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 07:36:19.867996: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "from torchtext.vocab import GloVe, FastText\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2169a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/Datasets/ClassificationDataset1.xlsx')\n",
    "#/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd3f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Negative Review  \\\n",
      "0  I am so angry that i made this post available ...   \n",
      "1                                        No Negative   \n",
      "2   Rooms are nice but for elderly a bit difficul...   \n",
      "3   My room was dirty and I was afraid to walk ba...   \n",
      "4  You When I booked with your company on line yo...   \n",
      "\n",
      "                                     Positive Review  \n",
      "0   Only the park outside of the hotel was beauti...  \n",
      "1   No real complaints the hotel was great great ...  \n",
      "2   Location was good and staff were ok It is cut...  \n",
      "3   Great location in nice surroundings the bar a...  \n",
      "4    Amazing location and building Romantic setting   \n",
      "(8999, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))\n",
    "print(data.shape)\n",
    "#df = df.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f6a3a",
   "metadata": {},
   "source": [
    "## Download Glove vectors and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bfa6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "fasttext = FastText(language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503745f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring Path \n",
    "#/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/Datasets/ClassificationDataset1.xlsx\n",
    "pos_path = '/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/data/positive_reviews.csv'\n",
    "neg_path = '/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/data/negative_reviews.csv'\n",
    "\n",
    "def making_new_dataset(data):\n",
    "    data.to_csv(pos_path, index = False, header= False,\n",
    "          encoding = \"latin-1\", columns = ['Positive Review'])\n",
    "    \n",
    "    data.to_csv(neg_path, index = False, header= False,\n",
    "          encoding = \"latin-1\", columns = ['Negative Review'])\n",
    "    \n",
    "    \n",
    "    positive_set = open(pos_path, \"r\", encoding=\"latin-1\").read()\n",
    "    negative_set = open(neg_path, \"r\", encoding=\"latin-1\").read()\n",
    "    \n",
    "    pos_set = positive_set.split(\"\\n\")[:-1]\n",
    "    neg_set = negative_set.split(\"\\n\")[:-1]\n",
    "    \n",
    "    #print(len(positive_data), len(negative_data))\n",
    "    \n",
    "    return pos_set, neg_set\n",
    "\n",
    "positive_data, negative_data = making_new_dataset(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b17617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ReviewsText(data, mode='positive'):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    data_set = []\n",
    "    vocabulary = {}\n",
    "    chars_to_remove = ['--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '.', ',']\n",
    "    \n",
    "    if mode == 'positive':\n",
    "        sentiment = 1\n",
    "    else :\n",
    "        sentiment = 0\n",
    "    \n",
    "    for i, v in enumerate(data):\n",
    "\n",
    "            # Removing Un-necessary symbols in our sentence\n",
    "            for chars in chars_to_remove:\n",
    "                v = v.replace(chars, \" \", -1)\n",
    "\n",
    "            sentence = torch.zeros(600)\n",
    "            n = 0\n",
    "            for token in nlp(v): \n",
    "\n",
    "                sentence[:300] += global_vectors.get_vecs_by_tokens(token.text, lower_case_backup=True)\n",
    "                sentence[300:] += fasttext.get_vecs_by_tokens(token.text, lower_case_backup=True).squeeze(dim=0)\n",
    "                n += 1\n",
    "\n",
    "            # Taking mean\n",
    "            sentence = sentence / n\n",
    "            data_set.append((sentence, sentiment))\n",
    "    \n",
    "    return data_set     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc73eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positive reviews : 8999 || Length of positive reviews : 8999\n",
      "Length of Complete concatenated Dataset of reviews : 17998\n"
     ]
    }
   ],
   "source": [
    "pos = preprocess_ReviewsText(positive_data, 'positive')\n",
    "neg = preprocess_ReviewsText(negative_data, 'negative')\n",
    "data = pos + neg\n",
    "print(f\"Length of positive reviews : {len(pos)} || Length of positive reviews : {len(neg)}\")\n",
    "print(f\"Length of Complete concatenated Dataset of reviews : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9951bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s %s' % x for x in data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cbf6b",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc8c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''\n",
    "        For Reproducibility: Sets the seed of the entire notebook.\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Sets a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def split_indices(num_values, percentage):\n",
    "\n",
    "    # Determine size of Validation set\n",
    "    val_size = int(percentage * num_values)\n",
    "\n",
    "    # Create random permutation of 0 to num_values-1\n",
    "    idxs = np.random.permutation(num_values)\n",
    "    return idxs[val_size:], idxs[:val_size]\n",
    "\n",
    "set_seed(1)\n",
    "train_pos_indices, val_pos_indices = split_indices(len(positive_data), 0.1)\n",
    "train_neg_indices, val_neg_indices = split_indices(len(negative_data), 0.1)\n",
    "\n",
    "train_indices = np.concatenate((train_pos_indices, train_neg_indices+len(positive_data)-1))\n",
    "val_indices = np.concatenate((val_pos_indices, val_neg_indices+len(positive_data)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c43405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1798, 16200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_indices), len(train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684c0ff",
   "metadata": {},
   "source": [
    "## Data Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c18481de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_fn(instn):\n",
    "\n",
    "    sentence = torch.zeros(len(instn), 600)\n",
    "\n",
    "    for i, v in enumerate(instn):\n",
    "        sentence[i] = v[0]\n",
    "\n",
    "    labels = torch.Tensor([x[1] for x in instn])\n",
    "\n",
    "    return (sentence, labels)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_sampler   = SubsetRandomSampler(train_indices)\n",
    "train_loader    = DataLoader(data, batch_size, sampler=train_sampler, collate_fn=batch_fn)\n",
    "\n",
    "val_sampler     = SubsetRandomSampler(val_indices)\n",
    "val_loader      = DataLoader(data, batch_size, sampler=val_sampler, collate_fn=batch_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ac805",
   "metadata": {},
   "source": [
    "# Models : DAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7942543",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c8f5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(600, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 2048)\n",
    "        self.linear3 = nn.Linear(2048, 512)\n",
    "        self.linear4 = nn.Linear(512, 64)\n",
    "        self.linear5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, Xb, tsne = False):\n",
    "        x = self.linear1(Xb)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.linear3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear4(x)\n",
    "        \n",
    "        if tsne == True:\n",
    "            return x\n",
    "            \n",
    "        x = F.relu(x)\n",
    "        x = self.linear5(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1398b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DAN()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "loss_fn = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adb1bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd47baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_predictions, plot_decision_boundary, accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed51f78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 440.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Training Loss:  69.30224609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1581.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.182392597198486\n",
      "Validation accuracy:  0.9327030033370411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 794.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  Training Loss:  44.79571533203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1693.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.104466915130615\n",
      "Validation accuracy:  0.9343715239154616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 793.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  Training Loss:  39.6877555847168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1461.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  4.939173221588135\n",
      "Validation accuracy:  0.9382647385984427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 798.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4  Training Loss:  36.58692169189453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1757.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  4.81107234954834\n",
      "Validation accuracy:  0.9365962180200222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 798.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  Training Loss:  34.09595489501953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1755.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.006438732147217\n",
      "Validation accuracy:  0.9410456062291435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 781.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6  Training Loss:  32.08406448364258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1455.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.043338775634766\n",
      "Validation accuracy:  0.9354838709677419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 778.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7  Training Loss:  29.650548934936523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1456.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.154064655303955\n",
      "Validation accuracy:  0.9421579532814238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 777.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  Training Loss:  28.457361221313477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1744.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.079061508178711\n",
      "Validation accuracy:  0.9449388209121246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 786.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9  Training Loss:  25.954675674438477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1456.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.203518390655518\n",
      "Validation accuracy:  0.9382647385984427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 784.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  Training Loss:  24.673669815063477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1751.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.115695476531982\n",
      "Validation accuracy:  0.9416017797552837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 797.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11  Training Loss:  23.90966033935547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1762.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.197197914123535\n",
      "Validation accuracy:  0.9388209121245829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 489.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12  Training Loss:  21.773103713989258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1767.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  6.209021091461182\n",
      "Validation accuracy:  0.9327030033370411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 799.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13  Training Loss:  20.886354446411133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1764.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.648735046386719\n",
      "Validation accuracy:  0.942714126807564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 798.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14  Training Loss:  18.311288833618164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1637.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  6.243546962738037\n",
      "Validation accuracy:  0.9443826473859844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 787.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15  Training Loss:  17.604734420776367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1459.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.542281150817871\n",
      "Validation accuracy:  0.9421579532814238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 806.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16  Training Loss:  17.30828285217285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1755.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.947999000549316\n",
      "Validation accuracy:  0.9449388209121246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 796.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17  Training Loss:  16.030742645263672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1756.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  5.999490737915039\n",
      "Validation accuracy:  0.9410456062291435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 787.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18  Training Loss:  14.291221618652344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1457.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  6.68242883682251\n",
      "Validation accuracy:  0.9421579532814238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 784.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19  Training Loss:  13.431424140930176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1459.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  7.287652492523193\n",
      "Validation accuracy:  0.9365962180200222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 766.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20  Training Loss:  12.97905158996582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1462.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  7.025003910064697\n",
      "Validation accuracy:  0.9443826473859844\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 773.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21  Training Loss:  11.838408470153809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1461.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  7.117300033569336\n",
      "Validation accuracy:  0.9438264738598443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 773.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  22  Training Loss:  10.930962562561035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1742.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  7.871668815612793\n",
      "Validation accuracy:  0.9410456062291435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 795.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23  Training Loss:  14.022547721862793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1753.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  7.981631278991699\n",
      "Validation accuracy:  0.9399332591768632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 763.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  24  Training Loss:  10.703930854797363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1415.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  8.328694343566895\n",
      "Validation accuracy:  0.9399332591768632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 254/254 [00:00<00:00, 477.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  25  Training Loss:  9.107155799865723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 1750.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  7.744479656219482\n",
      "Validation accuracy:  0.9371523915461624\n",
      "Saving Model\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 25\n",
    "\n",
    "best_test_acc = 0\n",
    "for ep in range(max_epoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for xb, yb in tqdm(train_loader):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        y_hat = model(xb)\n",
    "\n",
    "        loss = loss_fn(y_hat.squeeze(), yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    print(\"Epoch: \", ep+1, \" Training Loss: \", epoch_loss.item())\n",
    "    \n",
    "    #----------- Validation -----------\n",
    "\n",
    "    val_labels = []\n",
    "    val_pred = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_epoch_loss = 0\n",
    "\n",
    "    for xb, yb in tqdm(val_loader):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        y_hat = model(xb)\n",
    "\n",
    "        loss = loss_fn(y_hat.squeeze(), yb)\n",
    "\n",
    "        val_epoch_loss += loss\n",
    "\n",
    "        val_labels.extend(yb.cpu().detach().numpy())\n",
    "        val_pred.extend(y_hat.round().cpu().detach().numpy())\n",
    "\n",
    "    print(\"Validation loss: \", val_epoch_loss.item())\n",
    "    print(\"Validation accuracy: \", accuracy_score(val_labels, val_pred))\n",
    "\n",
    "\n",
    "    if ep > 15 and prev_val_loss - val_epoch_loss.item() > 0.05:\n",
    "        print(\"Saving Model\")\n",
    "        torch.save(model.state_dict(), \"DAN_model1.pt\")\n",
    "    \n",
    "    prev_val_loss = val_epoch_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d883506",
   "metadata": {},
   "source": [
    "## Model : LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2ef15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "\n",
    "def preprocess_tokenized_reviewText(data, mode='positive'):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    data_set = []\n",
    "    vocab = []\n",
    "    chars_to_remove = ['--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '.', ',']\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    \n",
    "    if mode == 'positive':\n",
    "        sentiment = 1\n",
    "    else :\n",
    "        sentiment = 0\n",
    "    \n",
    "    for line in data:\n",
    "        # Tokenizes the input text into words\n",
    "        tokens = tokenizer(line)\n",
    "\n",
    "        data_set.append((tokens, sentiment))\n",
    "        # Adds the extracted words to a list\n",
    "        vocab.extend(tokens)\n",
    "    print(f\"--- {mode} Finished ---\")\n",
    "    \n",
    "    return data_set\n",
    "\n",
    "#-----------------------------------------------------------------------------------#\n",
    "\n",
    "#len(set(vocab))\n",
    "def sort_key(s):\n",
    "    return len(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b66b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- positive Finished ---\n",
      "--- negative Finished ---\n",
      "Length of positive reviews : 8999 || Length of positive reviews : 8999\n",
      "Length of Complete concatenated Dataset of reviews : 17998\n"
     ]
    }
   ],
   "source": [
    "new_pos = preprocess_tokenized_reviewText(positive_data, 'positive')\n",
    "new_neg = preprocess_tokenized_reviewText(negative_data, 'negative')\n",
    "new_data = new_pos + new_neg\n",
    "print(f\"Length of positive reviews : {len(new_pos)} || Length of positive reviews : {len(new_neg)}\")\n",
    "print(f\"Length of Complete concatenated Dataset of reviews : {len(new_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758a055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
