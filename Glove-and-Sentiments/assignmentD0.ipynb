{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 15:13:05.672897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 15:13:06.178463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-18 15:13:06.790584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 15:13:06.790839: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "from torchtext.vocab import GloVe, FastText\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "import csv\n",
    "\n",
    "pos_set = []\n",
    "neg_set = []\n",
    "neutral_set = []\n",
    "with open(\"/data/home/ayyoobmohd/DLNLP/Glove-and-Sentiments/data/Dataset0.csv\", encoding='utf-8') as csvf:\n",
    "    data = csv.DictReader(csvf)\n",
    "    #data['Review'] = data['Review'].apply(lambda x:remove_punctuation(x))\n",
    "    for rows in data:\n",
    "        # Removing punctuations\n",
    "        chars_to_remove = [ '+', '#', '¡', '§', '…','‘', '’', '¿', '«', '»', '¨', '%', '-', '“', '”', '--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '/']\n",
    "        \n",
    "        review = rows['Review'].replace('<br />', \" \", -1)\n",
    "        review = review.replace('´', \"'\", -1)\n",
    "        for char in chars_to_remove:\n",
    "            review = review.replace(char, \" \", -1)\n",
    "        \n",
    "        \n",
    "        if rows['Label'] == 'positive':\n",
    "            pos_set.append(rows['Review'])\n",
    "        elif rows['Label'] == 'negative':\n",
    "            neg_set.append(rows['Review'])\n",
    "        else:\n",
    "            neutral_set.append(rows['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''\n",
    "        For Reproducibility: Sets the seed of the entire notebook.\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Sets a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Positive Finished ---\n",
      "--- Negative Finished ---\n",
      "--- Negative Finished ---\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# Downloads GloVe and FastText\n",
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "\n",
    "# ----------- Text Preprocessing -----------\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "data_set = []\n",
    "vocab = []\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "for line in pos_set:\n",
    "\n",
    "    # Tokenizes the input text into words\n",
    "    tokens = tokenizer(line)\n",
    "\n",
    "    data_set.append((tokens, 0))\n",
    "    # Adds the extracted words to a list\n",
    "    vocab.extend(tokens)\n",
    "\n",
    "\n",
    "print(\"--- Positive Finished ---\")\n",
    "\n",
    "for line in neg_set:\n",
    "\n",
    "    # Tokenizes the input text into words\n",
    "    tokens = tokenizer(line)\n",
    "\n",
    "    data_set.append((tokens, 1))\n",
    "    # Adds the extracted words to a list\n",
    "    vocab.extend(tokens)\n",
    "\n",
    "print(\"--- Negative Finished ---\")\n",
    "\n",
    "for line in neutral_set:\n",
    "\n",
    "    # Tokenizes the input text into words\n",
    "    tokens = tokenizer(line)\n",
    "\n",
    "    data_set.append((tokens, 2))\n",
    "    # Adds the extracted words to a list\n",
    "    vocab.extend(tokens)\n",
    "\n",
    "print(\"--- Negative Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the samples based on their sequence length\n",
    "def sort_key(s):\n",
    "    return len(s[0])\n",
    "    \n",
    "#data_set = sorted(data_set, key=sort_key)   # Sorting did not gave better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary:  9542\n"
     ]
    }
   ],
   "source": [
    "# Stores all the unique words in the dataset and their frequencies\n",
    "vocabulary = {}\n",
    "\n",
    "# Calculates the frequency of each unique word in the vocabulary\n",
    "for word in vocab:\n",
    "    if word in vocabulary:\n",
    "        vocabulary[word] += 1\n",
    "    else:\n",
    "        vocabulary[word] = 1\n",
    "\n",
    "print(\"Number of unique words in the vocabulary: \", len(vocabulary))\n",
    "\n",
    "# Stores the integer token for each unique word in the vocabulary\n",
    "ids_vocab = {}\n",
    "\n",
    "id = 0\n",
    "\n",
    "# Assigns words in the vocabulary to integer tokens\n",
    "for word, v in vocabulary.items():\n",
    "    ids_vocab[word] = id\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize(corpus, ids_vocab):\n",
    "    \"\"\"\n",
    "        Converts words in the dataset to integer tokens\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    for line, sentiment in corpus:\n",
    "        new_line = []\n",
    "        for i, word in enumerate(line):\n",
    "            if word in ids_vocab and (i == 0 or word != line[i-1]):\n",
    "                new_line.append(ids_vocab[word])\n",
    "\n",
    "        new_line = torch.Tensor(new_line).long()\n",
    "        tokenized_corpus.append((new_line, sentiment))\n",
    "\n",
    "    return tokenized_corpus\n",
    "\n",
    "token_corpus = tokenize(data_set, ids_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1739\n"
     ]
    }
   ],
   "source": [
    "# Loading the embedding matrix\n",
    "emb_dim = 300\n",
    "\n",
    "embeds = torch.zeros(len(ids_vocab) + 1, emb_dim)\n",
    "\n",
    "n = 0\n",
    "for token, idx in ids_vocab.items():\n",
    "    embeds[idx] = global_vectors[token]\n",
    "\n",
    "    if sum(embeds[idx]) == 0:\n",
    "        embeds[idx] = torch.rand(300)\n",
    "        n+=1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Valid split of 90-10\n",
    "def split_indices(n, val_pct):\n",
    "\n",
    "    # Determine size of Validation set\n",
    "    n_val = int(val_pct * n)\n",
    "\n",
    "    # Create random permutation of 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    #return np.sort(idxs[n_val:]), np.sort(idxs[:n_val])\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "train_pos_indices, val_pos_indices = split_indices(len(pos_set), 0.1)\n",
    "train_neg_indices, val_neg_indices = split_indices(len(neg_set), 0.1)\n",
    "train_neutral_indices, val_neutral_indices = split_indices(len(neutral_set), 0.1)\n",
    "\n",
    "# train_indices = np.concatenate((train_pos_indices, train_neg_indices+len(pos_set)-1))\n",
    "# val_indices = np.concatenate((val_pos_indices, val_neg_indices+len(pos_set)-1))\n",
    "train_indices = np.concatenate((train_pos_indices, train_neg_indices, train_neutral_indices))\n",
    "val_indices = np.concatenate((val_pos_indices, val_neg_indices, train_neutral_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ----------- Batching the data -----------\n",
    "def collate_fn(instn):\n",
    "\n",
    "    sentence = [x[0] for x in instn]\n",
    "\n",
    "    # Pre padding\n",
    "    sen_len = [len(x[0]) for x in instn]\n",
    "    max_len = max(sen_len)\n",
    "\n",
    "    padded_sent = torch.zeros(1, max_len)\n",
    "    sentence_pad = [torch.cat((torch.zeros(max_len-len(x[0])), x[0]), dim=0) for x in instn]\n",
    "    \n",
    "    for i in sentence_pad:\n",
    "        padded_sent = torch.cat((padded_sent, i.unsqueeze(dim=0)), dim=0)\n",
    "    padded_sent = padded_sent[1:].long()\n",
    "\n",
    "    # Post padding\n",
    "    #padded_sent = pad_sequence(sentence, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = torch.Tensor([x[1] for x in instn])\n",
    "\n",
    "    return (padded_sent, labels)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_sampler   = SubsetRandomSampler(train_indices)\n",
    "train_loader    = DataLoader(token_corpus, batch_size, sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "val_sampler     = SubsetRandomSampler(val_indices)\n",
    "val_loader      = DataLoader(token_corpus, batch_size, sampler=val_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Model -----------\n",
    "class BILSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeds):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeds, padding_idx=0)\n",
    "\n",
    "        self.gru = nn.GRU(input_size = 300, hidden_size = 128, num_layers = 2, batch_first = True, bidirectional = True, dropout=0.5)\n",
    "\n",
    "        self.lin1 = nn.Linear(256, 64)\n",
    "        self.lin2 = nn.Linear(64, 3)\n",
    "\n",
    "        self.lin3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        xe = self.embeddings(xb)\n",
    "        out, y = self.gru(xe)\n",
    "        \n",
    "        x = self.lin3(out).squeeze(dim=-1)\n",
    "        x = torch.softmax(x, dim=-1).unsqueeze(dim=1)\n",
    "        x = torch.bmm(x, out).squeeze(dim=1)              # Weighted average\n",
    "\n",
    "        #x = torch.cat((x, y[2][ :, :], y[3][ :, :]), dim = 1) # Tried concatenating the representation with hidden units - got similar results\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 212.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Training Loss:  0.9089754655443388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 372.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.9620582282543182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 212.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  Training Loss:  0.8552538711449196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 378.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.9572440594434738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 216.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  Training Loss:  0.8561467035063381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 381.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.9441723197698593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 216.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4  Training Loss:  0.846838603759634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 378.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.9134273618459702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 216.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  Training Loss:  0.8084861553948501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 376.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.8684382528066635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 217.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6  Training Loss:  0.7834040255382143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 380.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.835868564248085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 216.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7  Training Loss:  0.7661753790131931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 378.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.8389383971691131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 216.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  Training Loss:  0.7588861584663391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 374.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.7858832269906998\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 211.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9  Training Loss:  0.7388864488437258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 374.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.7638770163059234\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 212.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  Training Loss:  0.7118914415096415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 373.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.7311125338077545\n",
      "Saving Model\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "model = BILSTM(embeds)\n",
    "model.to(device)\n",
    "opt_c = torch.optim.AdamW(model.parameters(), lr = 0.001) # Same as Adam with weight decay = 0.001\n",
    "# loss_fn_c = F.cross_entropy #Tried Cross Entropy with log_softmax output function - gave similar results\n",
    "loss_fn_c = F.cross_entropy\n",
    "\n",
    "# ----------- Main Training Loop -----------\n",
    "max_epoch = 10\n",
    "\n",
    "best_test_acc = 0\n",
    "for ep in range(max_epoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for xb, yb in tqdm(train_loader):\n",
    "        yb = yb.type(torch.LongTensor)\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        \n",
    "        y_hat = model(xb)\n",
    "#         print(y_hat.shape)\n",
    "#         print(yb.shape)\n",
    "#         y_hat = torch.argmax(y_hat, dim =1)\n",
    "#         print(y_hat.shape)\n",
    "#         print(yb.shape)\n",
    "        loss = loss_fn_c(y_hat ,yb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt_c.step()\n",
    "\n",
    "        opt_c.zero_grad()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "        epoch_loss += float(loss)\n",
    "\n",
    "    print(\"Epoch: \", ep+1, \" Training Loss: \", epoch_loss/len(train_loader))\n",
    "\n",
    "\n",
    "    #----------- Validation -----------\n",
    "\n",
    "    val_labels = []\n",
    "    val_pred = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader):\n",
    "            yb = yb.type(torch.LongTensor)\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_hat = model(xb)\n",
    "#             y_hat = torch.argmax(y_hat, dim =1)\n",
    "            loss = loss_fn_c(y_hat,yb)\n",
    "\n",
    "            val_epoch_loss += float(loss)\n",
    "\n",
    "            val_labels.extend(torch.round(yb).cpu().detach().numpy())\n",
    "            val_pred.extend(y_hat.round().cpu().detach().numpy())\n",
    "\n",
    "    print(\"Validation loss: \", val_epoch_loss/len(val_loader))\n",
    "    #print(\"Validation accuracy: \", accuracy_score(val_labels, val_pred)*100)\n",
    "\n",
    "    if ep > 5 and prev_val_loss - val_epoch_loss > 0.015:\n",
    "        print(\"Saving Model\")\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    prev_val_loss = val_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(128,3)\n",
    "print(z.shape)\n",
    "y = torch.argmax(z, axis = 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_test(corpus, ids_vocab):\n",
    "    \"\"\"\n",
    "        Converts words in the dataset to integer tokens\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    for line, sentiment, idx in corpus:\n",
    "        new_line = []\n",
    "        for i, word in enumerate(line):\n",
    "            if word in ids_vocab and (i == 0 or word != line[i-1]):\n",
    "                new_line.append(ids_vocab[word])\n",
    "\n",
    "        new_line = torch.Tensor(new_line).long()\n",
    "        tokenized_corpus.append((new_line, sentiment, idx))\n",
    "\n",
    "    return tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Text Preprocessing ----------\n",
    "\n",
    "test_set = []\n",
    "with open(\"./E0334 Assignment2 Test Dataset.csv\", encoding='utf-8') as csvf:\n",
    "    data = csv.DictReader(csvf)\n",
    "\n",
    "    for idx, rows in enumerate(data):\n",
    "\n",
    "        # Removing punctuations\n",
    "        chars_to_remove = [ '+', '#', '¡', '§', '…','‘', '’', '¿', '«', '»', '¨', '%', '-', '“', '”', '--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '/']\n",
    "        \n",
    "        review = rows['review'].replace('<br />', \" \", -1)\n",
    "        review = review.replace('´', \"'\", -1)\n",
    "        for char in chars_to_remove:\n",
    "            review = review.replace(char, \" \", -1)\n",
    "\n",
    "        tokens = tokenizer(review)\n",
    "\n",
    "        if rows['sentiment'] == 'positive':\n",
    "            test_set.append((tokens, 1, idx))\n",
    "        else:\n",
    "            test_set.append((tokens, 0, idx))\n",
    "\n",
    "#test_set = sorted(test_set, key=sort_key)\n",
    "\n",
    "# ----------- Batching the data -----------\n",
    "def collate_fn_test(instn):\n",
    "\n",
    "    sentence = [x[0] for x in instn]\n",
    "\n",
    "    # Pre padding\n",
    "    sen_len = [len(x[0]) for x in instn]\n",
    "    max_len = max(sen_len)\n",
    "\n",
    "    padded_sent = torch.zeros(1, max_len)\n",
    "    sentence_pad = [torch.cat((torch.zeros(max_len-len(x[0])), x[0]), dim=0) for x in instn]\n",
    "    \n",
    "    for i in sentence_pad:\n",
    "        padded_sent = torch.cat((padded_sent, i.unsqueeze(dim=0)), dim=0)\n",
    "    padded_sent = padded_sent[1:].long()\n",
    "\n",
    "    # Post padding\n",
    "    #padded_sent = pad_sequence(sentence, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = torch.Tensor([x[1] for x in instn])\n",
    "\n",
    "    idx = torch.Tensor([x[2] for x in instn])\n",
    "\n",
    "    return (padded_sent, labels, idx)\n",
    "\n",
    "token_corpus_test = tokenize_test(test_set, ids_vocab)\n",
    "\n",
    "test_loader      = DataLoader(token_corpus_test, batch_size, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BILSTM(embeds)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.to(device)\n",
    "\n",
    "test_labels = []\n",
    "test_pred = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_epoch_loss = 0\n",
    "\n",
    "n = 0\n",
    "# ---------- Testing ----------\n",
    "with torch.no_grad():\n",
    "    for xb, yb, idx in tqdm(test_loader):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        y_hat = model(xb)\n",
    "        loss = loss_fn_c(y_hat.squeeze(), yb)\n",
    "\n",
    "        test_epoch_loss += float(loss)\n",
    "\n",
    "        test_labels.extend(torch.round(yb).cpu().detach().numpy())\n",
    "        test_pred.extend(y_hat.round().cpu().detach().numpy())\n",
    "\n",
    "        for i, v in enumerate(torch.round(yb).cpu().detach().numpy()):\n",
    "            if v != y_hat.round().cpu().detach().numpy()[i]:\n",
    "                print(test_set[int(idx[i])])\n",
    "                n += 1\n",
    "print(n, 79*128)\n",
    "print(\"Test loss: \", test_epoch_loss/len(test_loader))\n",
    "print(\"Test accuracy: \", accuracy_score(test_labels, test_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed doesn't work in Jupyter notebook, to replicate my results, kindly, run it as .py file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "82c36bf2dc7bc97628b9e43543d03433a2e60a09cf06bbc88105c7bffe751e99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
